{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6d5f0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "import json\n",
    "import argparse\n",
    "import logging\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24239038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "821b407b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taindp/miniconda3/envs/hipporag/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-05-20 14:16:50,289\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.hipporag import HippoRAG\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07d955af",
   "metadata": {},
   "outputs": [],
   "source": [
    "## clean cache\n",
    "import shutil\n",
    "shutil.rmtree(\"outputs\", ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b2161b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Encoding: 80it [00:06, 11.90it/s]                        \n",
      "NER: 100%|██████████| 75/75 [00:21<00:00,  3.42it/s, total_prompt_tokens=35514, total_completion_tokens=9571, num_cache_hit=0]\n",
      "Extracting triples: 100%|██████████| 75/75 [01:02<00:00,  1.20it/s, total_prompt_tokens=66041, total_completion_tokens=28941, num_cache_hit=0]\n",
      "Batch Encoding: 1344it [01:17, 17.35it/s]                          \n",
      "Batch Encoding: 1664it [01:26, 19.22it/s]                          \n",
      "75it [00:00, 18587.38it/s]\n",
      "75it [00:00, 52446.28it/s]\n",
      "KNN for Queries: 100%|██████████| 2/2 [00:00<00:00,  9.52it/s]\n",
      "100%|██████████| 1339/1339 [00:00<00:00, 72697.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_phrase_nodes': 1339, 'num_passage_nodes': 75, 'num_total_nodes': 1414, 'num_extracted_triples': 1654, 'num_triples_with_passage_node': 1797, 'num_synonymy_triples': 3201, 'num_total_triples': 6652}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "corpus_path = \"../data/courses_DS.json\"\n",
    "with open(corpus_path, \"r\") as f:\n",
    "    corpus = json.load(f)\n",
    "\n",
    "docs = [f\"{doc['title']}\\n{doc['text']}\" for doc in corpus]\n",
    "\n",
    "save_dir = 'outputs/openai_test'  # Define save directory for HippoRAG objects (each LLM/Embedding model combination will create a new subdirectory)\n",
    "llm_model_name = 'gpt-4o-mini'  # Any OpenAI model name\n",
    "embedding_model_name = 'text-embedding-3-small'  # Embedding model name (NV-Embed, GritLM or Contriever for now)\n",
    "\n",
    "# Startup a HippoRAG instance\n",
    "hipporag = HippoRAG(save_dir=save_dir,\n",
    "                    llm_model_name=llm_model_name,\n",
    "                    embedding_model_name=embedding_model_name,\n",
    "                    dataset=\"hcmus\" ## HippoRAG base\n",
    "                    )\n",
    "\n",
    "# Run indexing\n",
    "hipporag.index(docs=docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0bf880d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb86b9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_end_qa_ds = pd.DataFrame(json.load(open(\"../data/open_end_qa_ds.json\", \"r\")))\n",
    "queries = open_end_qa_ds[\"question\"].tolist()\n",
    "references = open_end_qa_ds[\"answer\"].tolist()\n",
    "gold_docs = [[f\"{item[0]['title']}\\n{item[0]['text']}\"] for item in open_end_qa_ds[\"paragraphs\"].tolist()]\n",
    "# gold_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a58b0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gold_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be95cf1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Encoding: 32it [00:01, 25.56it/s]                        \n",
      "Batch Encoding: 32it [00:01, 22.38it/s]                        \n",
      "Retrieving: 100%|██████████| 25/25 [00:51<00:00,  2.06s/it]\n",
      "Length of retrieved docs (75) is smaller than largest topk for recall score (200)\n",
      "Length of retrieved docs (75) is smaller than largest topk for recall score (200)\n",
      "Length of retrieved docs (75) is smaller than largest topk for recall score (200)\n",
      "Length of retrieved docs (75) is smaller than largest topk for recall score (200)\n",
      "Length of retrieved docs (75) is smaller than largest topk for recall score (200)\n",
      "Length of retrieved docs (75) is smaller than largest topk for recall score (200)\n",
      "Length of retrieved docs (75) is smaller than largest topk for recall score (200)\n",
      "Length of retrieved docs (75) is smaller than largest topk for recall score (200)\n",
      "Length of retrieved docs (75) is smaller than largest topk for recall score (200)\n",
      "Length of retrieved docs (75) is smaller than largest topk for recall score (200)\n",
      "Length of retrieved docs (75) is smaller than largest topk for recall score (200)\n",
      "Length of retrieved docs (75) is smaller than largest topk for recall score (200)\n",
      "Length of retrieved docs (75) is smaller than largest topk for recall score (200)\n",
      "Length of retrieved docs (75) is smaller than largest topk for recall score (200)\n",
      "Length of retrieved docs (75) is smaller than largest topk for recall score (200)\n",
      "Length of retrieved docs (75) is smaller than largest topk for recall score (200)\n",
      "Length of retrieved docs (75) is smaller than largest topk for recall score (200)\n",
      "Length of retrieved docs (75) is smaller than largest topk for recall score (200)\n",
      "Length of retrieved docs (75) is smaller than largest topk for recall score (200)\n",
      "Length of retrieved docs (75) is smaller than largest topk for recall score (200)\n",
      "Length of retrieved docs (75) is smaller than largest topk for recall score (200)\n",
      "Length of retrieved docs (75) is smaller than largest topk for recall score (200)\n",
      "Length of retrieved docs (75) is smaller than largest topk for recall score (200)\n",
      "Length of retrieved docs (75) is smaller than largest topk for recall score (200)\n",
      "Length of retrieved docs (75) is smaller than largest topk for recall score (200)\n",
      "Collecting QA prompts: 100%|██████████| 25/25 [00:00<00:00, 3141.42it/s]\n",
      "QA Reading: 100%|██████████| 25/25 [02:09<00:00,  5.18s/it]\n",
      "Extraction Answers from LLM Response: 25it [00:00, 106023.86it/s]\n"
     ]
    }
   ],
   "source": [
    "queries_solutions, all_response_message, all_metadata, overall_retrieval_result, overall_qa_results = hipporag.rag_qa(\n",
    "    queries=queries,\n",
    "    gold_docs=gold_docs,\n",
    "    gold_answers=references\n",
    ")\n",
    "# predictions = [item.split(\"Answer: \")[1] for item in all_response_message]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76c3780e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Recall@1': 0.36,\n",
       " 'Recall@2': 0.52,\n",
       " 'Recall@5': 0.68,\n",
       " 'Recall@10': 0.72,\n",
       " 'Recall@20': 0.72,\n",
       " 'Recall@30': 0.72,\n",
       " 'Recall@50': 0.72,\n",
       " 'Recall@100': 0.72,\n",
       " 'Recall@150': 0.72,\n",
       " 'Recall@200': 0.72}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_retrieval_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hipporag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
