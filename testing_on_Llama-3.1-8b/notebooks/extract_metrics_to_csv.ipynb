{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import os\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Datasets: ['AM', 'DS', 'MCS']\n",
            "Test types: ['closed_end', 'opened_end', 'multihop', 'multihop2']\n",
            "Metrics: ['avg_exact_match', 'avg_f1_score', 'avg_bleu4', 'avg_meteor', 'avg_rouge_l']\n"
          ]
        }
      ],
      "source": [
        "# Define paths\n",
        "base_path = Path(\"main_evaluation\")\n",
        "output_path = Path(\"evaluation_metrics_by_test_types.csv\")\n",
        "\n",
        "# Define datasets and test types\n",
        "datasets = [\"AM\", \"DS\", \"MCS\"]\n",
        "test_types = [\"closed_end\", \"opened_end\", \"multihop2\"]\n",
        "\n",
        "# Define metrics to extract\n",
        "metrics = [\n",
        "    \"avg_exact_match\",\n",
        "    \"avg_f1_score\", \n",
        "    \"avg_bleu4\",\n",
        "    \"avg_meteor\",\n",
        "    \"avg_rouge_l\"\n",
        "]\n",
        "\n",
        "print(f\"Datasets: {datasets}\")\n",
        "print(f\"Test types: {test_types}\")\n",
        "print(f\"Metrics: {metrics}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Functions defined successfully!\n"
          ]
        }
      ],
      "source": [
        "def read_json_file(file_path):\n",
        "    \"\"\"Read JSON file and return parsed content\"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            return json.load(f)\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_metrics_from_json(data, test_type):\n",
        "    \"\"\"Extract metrics for specific test type from JSON data\"\"\"\n",
        "    try:\n",
        "        # Navigate to the specific test type results\n",
        "        individual_results = data.get(\"individual_results\", {})\n",
        "        test_data = individual_results.get(test_type, {})\n",
        "        aggregate_metrics = test_data.get(\"aggregate_metrics\", {})\n",
        "        \n",
        "        # Extract required metrics\n",
        "        extracted = {}\n",
        "        for metric in metrics:\n",
        "            extracted[metric] = aggregate_metrics.get(metric, 0.0)\n",
        "        \n",
        "        return extracted\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting metrics for {test_type}: {e}\")\n",
        "        return {metric: 0.0 for metric in metrics}\n",
        "\n",
        "def process_dataset(dataset_name):\n",
        "    \"\"\"Process all test types for a specific dataset\"\"\"\n",
        "    file_path = base_path / f\"{dataset_name}_all_test_types_summary.json\"\n",
        "    \n",
        "    if not file_path.exists():\n",
        "        print(f\"File not found: {file_path}\")\n",
        "        return []\n",
        "    \n",
        "    print(f\"Processing {dataset_name}...\")\n",
        "    \n",
        "    # Read JSON data\n",
        "    data = read_json_file(file_path)\n",
        "    if data is None:\n",
        "        return []\n",
        "    \n",
        "    results = []\n",
        "    \n",
        "    # Extract metrics for each test type\n",
        "    for test_type in test_types:\n",
        "        metrics_data = extract_metrics_from_json(data, test_type)\n",
        "        \n",
        "        row = {\n",
        "            \"Dataset\": dataset_name,\n",
        "            \"Test_Type\": test_type,\n",
        "            \"Exact_Match\": metrics_data[\"avg_exact_match\"],\n",
        "            \"F1_Score\": metrics_data[\"avg_f1_score\"],\n",
        "            \"BLEU_4\": metrics_data[\"avg_bleu4\"],\n",
        "            \"METEOR\": metrics_data[\"avg_meteor\"],\n",
        "            \"ROUGE_L\": metrics_data[\"avg_rouge_l\"]\n",
        "        }\n",
        "        \n",
        "        results.append(row)\n",
        "        print(f\"  {test_type}: EM={row['Exact_Match']:.4f}, F1={row['F1_Score']:.4f}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "print(\"Functions defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Directory not found: main_evaluation\n",
            "Available directories:\n",
            "  output_logs\n"
          ]
        }
      ],
      "source": [
        "# Check if main_evaluation directory exists\n",
        "if not base_path.exists():\n",
        "    print(f\"Directory not found: {base_path}\")\n",
        "    print(\"Available directories:\")\n",
        "    for item in Path(\".\").iterdir():\n",
        "        if item.is_dir():\n",
        "            print(f\"  {item}\")\n",
        "else:\n",
        "    print(f\"Found directory: {base_path}\")\n",
        "    print(\"Available files:\")\n",
        "    for item in base_path.iterdir():\n",
        "        if item.is_file() and item.suffix == '.json':\n",
        "            print(f\"  {item.name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File not found: main_evaluation\\AM_all_test_types_summary.json\n",
            "Completed AM\n",
            "\n",
            "File not found: main_evaluation\\DS_all_test_types_summary.json\n",
            "Completed DS\n",
            "\n",
            "File not found: main_evaluation\\MCS_all_test_types_summary.json\n",
            "Completed MCS\n",
            "\n",
            "Total rows extracted: 0\n"
          ]
        }
      ],
      "source": [
        "# Process all datasets\n",
        "all_results = []\n",
        "\n",
        "for dataset in datasets:\n",
        "    dataset_results = process_dataset(dataset)\n",
        "    all_results.extend(dataset_results)\n",
        "    print(f\"Completed {dataset}\\n\")\n",
        "\n",
        "print(f\"Total rows extracted: {len(all_results)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DataFrame shape: (0, 0)\n",
            "Columns: []\n",
            "\n",
            "DataFrame preview:\n",
            "Empty DataFrame\n",
            "Columns: []\n",
            "Index: []\n"
          ]
        }
      ],
      "source": [
        "# Create DataFrame\n",
        "df = pd.DataFrame(all_results)\n",
        "\n",
        "# Display basic info\n",
        "print(f\"DataFrame shape: {df.shape}\")\n",
        "print(f\"Columns: {list(df.columns)}\")\n",
        "print(\"\\nDataFrame preview:\")\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data saved to: evaluation_metrics_by_test_types.csv\n",
            "File size: 2 bytes\n",
            "\n",
            "First few lines of saved file:\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Save to CSV\n",
        "df.to_csv(output_path, index=False, float_format='%.4f')\n",
        "print(f\"Data saved to: {output_path}\")\n",
        "\n",
        "# Verify file was created\n",
        "if output_path.exists():\n",
        "    file_size = output_path.stat().st_size\n",
        "    print(f\"File size: {file_size} bytes\")\n",
        "    \n",
        "    # Read back and display first few lines\n",
        "    print(\"\\nFirst few lines of saved file:\")\n",
        "    with open(output_path, 'r') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if i < 5:\n",
        "                print(line.strip())\n",
        "            else:\n",
        "                break\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "print(df.columns.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== AVERAGE METRICS BY DATASET ===\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "'Dataset'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[16], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Summary by dataset\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== AVERAGE METRICS BY DATASET ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m summary_by_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDataset\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExact_Match\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF1_Score\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBLEU_4\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMETEOR\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mROUGE_L\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(summary_by_dataset\u001b[38;5;241m.\u001b[39mround(\u001b[38;5;241m4\u001b[39m))\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== AVERAGE METRICS BY TEST TYPE ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:9183\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[1;34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[0;32m   9180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   9181\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to supply one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mby\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 9183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   9184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9186\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9187\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9189\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9193\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1329\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[1;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropna \u001b[38;5;241m=\u001b[39m dropna\n\u001b[0;32m   1328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1329\u001b[0m     grouper, exclusions, obj \u001b[38;5;241m=\u001b[39m \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1332\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1334\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mno_default\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1336\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1337\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[0;32m   1340\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ping\u001b[38;5;241m.\u001b[39m_passed_categorical \u001b[38;5;28;01mfor\u001b[39;00m ping \u001b[38;5;129;01min\u001b[39;00m grouper\u001b[38;5;241m.\u001b[39mgroupings):\n",
            "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\groupby\\grouper.py:1043\u001b[0m, in \u001b[0;36mget_grouper\u001b[1;34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[0m\n\u001b[0;32m   1041\u001b[0m         in_axis, level, gpr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1042\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1043\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[0;32m   1044\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1045\u001b[0m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[0;32m   1046\u001b[0m     exclusions\u001b[38;5;241m.\u001b[39madd(gpr\u001b[38;5;241m.\u001b[39mkey)\n",
            "\u001b[1;31mKeyError\u001b[0m: 'Dataset'"
          ]
        }
      ],
      "source": [
        "# Summary by dataset\n",
        "print(\"=== AVERAGE METRICS BY DATASET ===\")\n",
        "summary_by_dataset = df.groupby('Dataset')[['Exact_Match', 'F1_Score', 'BLEU_4', 'METEOR', 'ROUGE_L']].mean()\n",
        "print(summary_by_dataset.round(4))\n",
        "\n",
        "print(\"\\n=== AVERAGE METRICS BY TEST TYPE ===\")\n",
        "summary_by_test = df.groupby('Test_Type')[['Exact_Match', 'F1_Score', 'BLEU_4', 'METEOR', 'ROUGE_L']].mean()\n",
        "print(summary_by_test.round(4))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== EXACT MATCH BY DATASET AND TEST TYPE ===\n",
            "Dataset         AM   DS   MCS\n",
            "Test_Type                    \n",
            "closed_end  0.6731  0.6  0.76\n",
            "multihop    0.0000  0.0  0.00\n",
            "multihop2   0.0000  0.0  0.00\n",
            "opened_end  0.0000  0.0  0.00\n",
            "\n",
            "=== F1 SCORE BY DATASET AND TEST TYPE ===\n",
            "Dataset         AM      DS     MCS\n",
            "Test_Type                         \n",
            "closed_end  0.7207  0.6795  0.8217\n",
            "multihop    0.4199  0.4489  0.4973\n",
            "multihop2   0.4637  0.4089  0.4332\n",
            "opened_end  0.5658  0.3984  0.5546\n",
            "\n",
            "=== ADDITIONAL FILES CREATED ===\n",
            "- summary_by_dataset.csv\n",
            "- summary_by_test_type.csv\n",
            "- exact_match_pivot.csv\n",
            "- f1_score_pivot.csv\n"
          ]
        }
      ],
      "source": [
        "# Create pivot tables for better visualization\n",
        "print(\"=== EXACT MATCH BY DATASET AND TEST TYPE ===\")\n",
        "pivot_em = df.pivot(index='Test_Type', columns='Dataset', values='Exact_Match')\n",
        "print(pivot_em.round(4))\n",
        "\n",
        "print(\"\\n=== F1 SCORE BY DATASET AND TEST TYPE ===\")\n",
        "pivot_f1 = df.pivot(index='Test_Type', columns='Dataset', values='F1_Score')\n",
        "print(pivot_f1.round(4))\n",
        "\n",
        "# Save summary tables\n",
        "summary_by_dataset.to_csv(\"summary_by_dataset.csv\", float_format='%.4f')\n",
        "summary_by_test.to_csv(\"summary_by_test_type.csv\", float_format='%.4f')\n",
        "pivot_em.to_csv(\"exact_match_pivot.csv\", float_format='%.4f')\n",
        "pivot_f1.to_csv(\"f1_score_pivot.csv\", float_format='%.4f')\n",
        "\n",
        "print(\"\\n=== ADDITIONAL FILES CREATED ===\")\n",
        "print(\"- summary_by_dataset.csv\")\n",
        "print(\"- summary_by_test_type.csv\")\n",
        "print(\"- exact_match_pivot.csv\")\n",
        "print(\"- f1_score_pivot.csv\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
